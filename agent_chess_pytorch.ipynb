{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from gym_chess_env import ChessBoard_gym\n",
    "import wandb\n",
    "from onecyclelr import OneCycleLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: pnarsina (use `wandb login --relogin` to force relogin)\n",
      "wandb: wandb version 0.11.1 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.32<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">tough-moon-39</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/pnarsina/w251-prabhu-final_chessproject-notebook\" target=\"_blank\">https://wandb.ai/pnarsina/w251-prabhu-final_chessproject-notebook</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/pnarsina/w251-prabhu-final_chessproject-notebook/runs/1x810ckv\" target=\"_blank\">https://wandb.ai/pnarsina/w251-prabhu-final_chessproject-notebook/runs/1x810ckv</a><br/>\n",
       "                Run data is saved locally in <code>C:\\prabhu\\edu\\code\\w251\\myrepo\\chess_project\\wandb\\run-20210801_161822-1x810ckv</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(1x810ckv)</h1><iframe src=\"https://wandb.ai/pnarsina/w251-prabhu-final_chessproject-notebook/runs/1x810ckv\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1cbc49b8280>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()\n",
    "wandb.init(project=\"w251-prabhu-final_chessproject-notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ChessBoard_gym()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.DataParallel(nn.Linear(64, 128))\n",
    "        self.fc2 = nn.DataParallel(nn.Linear(128, 512))\n",
    "        self.fc3 = nn.DataParallel(nn.Linear(512, 256))\n",
    "        self.fc4 = nn.DataParallel(nn.Linear(256, 128))\n",
    "#         self.bn1 = nn.BatchNorm1d(128)\n",
    "#         self.conv1 = nn.Conv1d(16, 16, kernel_size=3, stride=1)\n",
    "#         self.bn1 = nn.BatchNorm1d(16)\n",
    "#         self.conv2 = nn.Conv1d(16, 32, kernel_size=5, stride=2)\n",
    "#         self.bn2 = nn.BatchNorm1d(32)\n",
    "#         self.conv3 = nn.Conv1d(32, 16, kernel_size=5, stride=2)\n",
    "#         self.bn3 = nn.BatchNorm1d(16)\n",
    "\n",
    "#         # Number of Linear input connections depends on output of conv2d layers\n",
    "#         # and therefore the input image size, so compute it.\n",
    "#         def conv2d_size_out(size, kernel_size = 5, stride = ):\n",
    "#             return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        \n",
    "#         def conv1d_size_out(size, kernel_size = 5, stride = 2):\n",
    "#             return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "\n",
    "#         convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "#         convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "#         linear_input_size = convw * convh * 32\n",
    "        self.head = nn.DataParallel(nn.Linear(128, outputs))\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "#     def forward(self, x):\n",
    "#         x = x.to(device)\n",
    "#         x = F.relu(self.conv1(x))\n",
    "# #         x = F.relu(self.bn1(self.conv1(x)))\n",
    "# #         x = F.relu(self.bn2(self.conv2(x)))\n",
    "# #         x = F.relu(self.bn3(self.conv3(x)))\n",
    "#         return self.head(x.view(x.size(0), -1))\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "br1 - BL - BL - BL - BL - BL - BL - br2 -  \n",
      "\n",
      "BL - BL - BL - BL - BL - BL - BL - BL -  \n",
      "\n",
      "BL - BL - BL - BL - BL - BL - BL - BL -  \n",
      "\n",
      "BL - BL - BL - BL - BL - BL - BL - BL -  \n",
      "\n",
      "BL - BL - BL - BL - BL - BL - BL - BL -  \n",
      "\n",
      "BL - BL - BL - BL - BL - BL - BL - BL -  \n",
      "\n",
      "BL - BL - BL - BL - BL - BL - BL - BL -  \n",
      "\n",
      "wr1 - BL - BL - BL - BL - BL - BL - wr2 -  \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.print_board()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chess Cells 8 x 8\n",
    "height = 8 \n",
    "width = 8\n",
    "n_actions = env.action_space.n\n",
    "# n_actions = 112\n",
    "BATCH_SIZE = 1024\n",
    "GAMMA = 0.999\n",
    "EPS_START = 1 \n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 500000\n",
    "TARGET_UPDATE = 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = DQN(height, width, n_actions).to(device)\n",
    "target_net = DQN(height, width, n_actions).to(device)\n",
    "# policy_net = DQN(n_actions).to(device)\n",
    "# target_net = DQN(n_actions).to(device)\n",
    "\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10)\n",
    "# optimizer = optim.AdamW(policy_net.parameters())\n",
    "# optimizer = optim.SGD(lr=0.0001 )\n",
    "memory = ReplayMemory(500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    global steps_done\n",
    "    \n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    \n",
    "#     print(\"epsilon threshold:\", eps_threshold)\n",
    "    \n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "#         plt.plot(means.numpy())\n",
    "\n",
    "#     plt.pause(0.5)  # pause a bit so that plots are updated\n",
    "#     if is_ipython:\n",
    "#         display.clear_output(wait=True)\n",
    "#         display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "\n",
    "    pred = output \n",
    "    correct = pred.eq(target)\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k)\n",
    "    return res\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
    "\n",
    "# In[13]:\n",
    "def save_checkpoint(state,  filename='checkpoint.pth.tar'):\n",
    "    # save the model state!\n",
    "    torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return(0,0,0)\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "  \n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    acc1, acc5 = accuracy(state_action_values,  expected_state_action_values.unsqueeze(1), topk=(1, 5)) \n",
    "   # print('accuracies ', acc1, \" \", acc5)\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    return (loss,acc1,acc5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss of the episode  0  is : 0  acc1: 0 acc5: 0  Batch size: 1024  Memory size: 798 reward mean: -398.0\n",
      "loss of the episode  1  is : tensor(6896.9663, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 1585 reward mean: -392.5\n",
      "loss of the episode  2  is : tensor(4.4392, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 2290 reward mean: -363.3333333333333\n",
      "loss of the episode  3  is : tensor(6.2647, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 3068 reward mean: -367.0\n",
      "loss of the episode  4  is : tensor(8.1631, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 3894 reward mean: -378.8\n",
      "loss of the episode  5  is : tensor(18.0166, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 4904 reward mean: -417.3333333333333\n",
      "loss of the episode  6  is : tensor(29.9404, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 6058 reward mean: -465.42857142857144\n",
      "loss of the episode  7  is : tensor(27.4010, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 6864 reward mean: -458.0\n",
      "loss of the episode  8  is : tensor(25.9293, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 7572 reward mean: -441.3333333333333\n",
      "loss of the episode  9  is : tensor(26.1949, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 8359 reward mean: -435.9\n",
      "loss of the episode  10  is : tensor(26.2577, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 9084 reward mean: -425.8181818181818\n",
      "loss of the episode  11  is : tensor(229.2930, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 9824 reward mean: -418.6666666666667\n",
      "loss of the episode  12  is : tensor(89.2832, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 10614 reward mean: -416.46153846153845\n",
      "loss of the episode  13  is : tensor(76.1036, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 11471 reward mean: -419.35714285714283\n",
      "loss of the episode  14  is : tensor(63.0098, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 12219 reward mean: -414.6\n",
      "loss of the episode  15  is : tensor(66.8201, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 13026 reward mean: -414.125\n",
      "loss of the episode  16  is : tensor(65.2800, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 13797 reward mean: -411.5882352941176\n",
      "loss of the episode  17  is : tensor(294.6938, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 17420 reward mean: -567.7777777777778\n",
      "loss of the episode  18  is : tensor(52.5353, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 18153 reward mean: -555.421052631579\n",
      "loss of the episode  19  is : tensor(59.8843, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 18874 reward mean: -543.7\n",
      "loss of the episode  20  is : tensor(69.0164, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 19612 reward mean: -533.9047619047619\n",
      "loss of the episode  21  is : tensor(2361.5994, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 20916 reward mean: -550.7272727272727\n",
      "loss of the episode  22  is : tensor(679.2565, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 21626 reward mean: -540.2608695652174\n",
      "loss of the episode  23  is : tensor(578.9261, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 22360 reward mean: -531.6666666666666\n",
      "loss of the episode  24  is : tensor(975.4991, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 23837 reward mean: -553.48\n",
      "loss of the episode  25  is : tensor(459.2718, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 24605 reward mean: -546.3461538461538\n",
      "loss of the episode  26  is : tensor(480.6132, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 25417 reward mean: -541.3703703703703\n",
      "loss of the episode  27  is : tensor(678.9520, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 26610 reward mean: -550.3571428571429\n",
      "loss of the episode  28  is : tensor(427.3672, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 27372 reward mean: -543.8620689655172\n",
      "loss of the episode  29  is : tensor(486.0222, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 28178 reward mean: -539.2666666666667\n",
      "loss of the episode  30  is : tensor(498.6422, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 28950 reward mean: -533.8709677419355\n",
      "loss of the episode  31  is : tensor(3961.7791, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 29668 reward mean: -527.125\n",
      "loss of the episode  32  is : tensor(4437.4121, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 30632 reward mean: -528.2424242424242\n",
      "loss of the episode  33  is : tensor(2942.2292, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 31381 reward mean: -522.9705882352941\n",
      "loss of the episode  34  is : tensor(4312.1255, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 32652 reward mean: -532.9142857142857\n",
      "loss of the episode  35  is : tensor(2263.2083, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 33400 reward mean: -527.7777777777778\n",
      "loss of the episode  36  is : tensor(4314.7510, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 34981 reward mean: -545.4324324324324\n",
      "loss of the episode  37  is : tensor(1744.6565, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 35675 reward mean: -538.8157894736842\n",
      "loss of the episode  38  is : tensor(1763.8754, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 36387 reward mean: -533.0\n",
      "loss of the episode  39  is : tensor(1899.4968, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 37151 reward mean: -528.775\n",
      "loss of the episode  40  is : tensor(1715.9769, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 37837 reward mean: -522.8536585365854\n",
      "loss of the episode  41  is : tensor(5917., device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 38547 reward mean: -517.7857142857143\n",
      "loss of the episode  42  is : tensor(4903.7344, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 39142 reward mean: -510.27906976744185\n",
      "loss of the episode  43  is : tensor(9464.9189, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 40312 reward mean: -516.1818181818181\n",
      "loss of the episode  44  is : tensor(5274.0801, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 40982 reward mean: -510.7111111111111\n",
      "loss of the episode  45  is : tensor(5971.3145, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 41744 reward mean: -507.4782608695652\n",
      "loss of the episode  46  is : tensor(9425.0342, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 42973 reward mean: -514.3191489361702\n",
      "loss of the episode  47  is : tensor(5234.0293, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 43673 reward mean: -509.8541666666667\n",
      "loss of the episode  48  is : tensor(5303.7686, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 44388 reward mean: -505.8775510204082\n",
      "loss of the episode  49  is : tensor(5261.7300, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 45100 reward mean: -502.0\n",
      "loss of the episode  50  is : tensor(4886.2754, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 45761 reward mean: -499.26\n",
      "loss of the episode  51  is : tensor(6876.9941, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 46508 reward mean: -498.46\n",
      "loss of the episode  52  is : tensor(6970.3350, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 47267 reward mean: -499.54\n",
      "loss of the episode  53  is : tensor(6124.0669, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 47933 reward mean: -497.3\n",
      "loss of the episode  54  is : tensor(6686.0688, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 48660 reward mean: -495.32\n",
      "loss of the episode  55  is : tensor(7016.5244, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 49421 reward mean: -490.34\n",
      "loss of the episode  56  is : tensor(6504.5698, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 50126 reward mean: -481.36\n",
      "loss of the episode  57  is : tensor(9756.5703, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 51187 reward mean: -486.46\n",
      "loss of the episode  58  is : tensor(5732.8394, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 51813 reward mean: -484.82\n",
      "loss of the episode  59  is : tensor(10245.4863, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 52934 reward mean: -491.5\n",
      "loss of the episode  60  is : tensor(6537.3032, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 53651 reward mean: -491.34\n",
      "loss of the episode  61  is : tensor(7033.7207, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 54405 reward mean: -491.62\n",
      "loss of the episode  62  is : tensor(6904.3965, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 55146 reward mean: -490.64\n",
      "loss of the episode  63  is : tensor(10068.7295, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 56229 reward mean: -495.16\n",
      "loss of the episode  64  is : tensor(11244.8428, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 57441 reward mean: -504.44\n",
      "loss of the episode  65  is : tensor(5609.8081, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 58046 reward mean: -500.4\n",
      "loss of the episode  66  is : tensor(6634.3306, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 58758 reward mean: -499.22\n",
      "loss of the episode  67  is : tensor(6618.7446, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 59470 reward mean: -441.0\n",
      "loss of the episode  68  is : tensor(9699.2490, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 60511 reward mean: -447.16\n",
      "loss of the episode  69  is : tensor(6781.3271, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 61241 reward mean: -447.34\n",
      "loss of the episode  70  is : tensor(6484.0942, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 61935 reward mean: -446.46\n",
      "loss of the episode  71  is : tensor(6531.3433, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 62636 reward mean: -434.4\n",
      "loss of the episode  72  is : tensor(11303.9277, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 63846 reward mean: -444.4\n",
      "loss of the episode  73  is : tensor(6171.6543, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 64510 reward mean: -443.0\n",
      "loss of the episode  74  is : tensor(6632.2119, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 1024  Memory size: 65224 reward mean: -427.74\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 100\n",
    "steps_done = 0\n",
    "observation_space = 64\n",
    "episode_durations = []\n",
    "rewards_list = []\n",
    "for i_episode in range(num_episodes):\n",
    "    reward_for_episode = 0\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    data_time = AverageMeter('Data', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    progress = ProgressMeter(\n",
    "        len(memory),\n",
    "        [batch_time, data_time, losses, top1, top5],\n",
    "        prefix=\"Epoch: [{}]\".format(i_episode))\n",
    "\n",
    "    # Initialize the environment and state\n",
    "    state = torch.from_numpy(env.reset()).float()\n",
    "    total_loss = 0\n",
    "    loss = 0\n",
    "    t=0\n",
    "    scheduler.step()\n",
    "    end = time.time()\n",
    "    legal_move_count=0\n",
    "    tot_acc1 = 0\n",
    "    tot_acc5 = 0\n",
    "    while True:\n",
    "        # Select and perform an action\n",
    "#         state_model_input = np.reshape(state, state.shape + (1,)) \n",
    "        state_model_input = torch.reshape(state, [1, observation_space])\n",
    "        \n",
    "        action = select_action(state_model_input)\n",
    "        next_state, reward, done, info = env.step(action.item())\n",
    "        \n",
    "        reward_for_episode += reward\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        \n",
    "       \n",
    "        # Store the transition in memory\n",
    "        next_state = torch.from_numpy(next_state).float()\n",
    "        \n",
    "#         next_state_model_input = np.reshape(next_state, next_state.shape + (1,)) \n",
    "        next_state_model_input = torch.reshape(next_state, [1, observation_space])\n",
    "    \n",
    "        memory.push(state_model_input, action, next_state_model_input, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        loss, acc1, acc5 = optimize_model()\n",
    "\n",
    "\n",
    "        if loss is not None: \n",
    "            total_loss = total_loss + loss\n",
    "            tot_acc1 = tot_acc1 + acc1\n",
    "            tot_acc5 = tot_acc5 + acc5\n",
    "\n",
    "            # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "        if done:\n",
    "            legal_move_count += 1\n",
    "\n",
    "        if legal_move_count >=200:\n",
    "            episode_durations.append(t + 1)\n",
    "            t+=1\n",
    "\n",
    "            if loss is not None: \n",
    "                total_loss = total_loss + loss\n",
    "                losses.update(total_loss, BATCH_SIZE)\n",
    "                top1.update(tot_acc1,  BATCH_SIZE)\n",
    "                top5.update(tot_acc5, BATCH_SIZE)\n",
    "            #plot_durations()\n",
    "            break\n",
    "            \n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    rewards_list.append(reward_for_episode)\n",
    "    last_rewards_mean = np.mean(rewards_list[-50:])\n",
    "    print (\"loss of the episode \", i_episode , \" is :\", total_loss, \" acc1:\", tot_acc1, \"acc5:\", tot_acc5, \" Batch size:\",BATCH_SIZE , \" Memory size:\", len(memory), \"reward mean:\", last_rewards_mean)\n",
    "    wandb.log({\"Loss/val\": losses.avg, 'acc1/val': top1.avg, 'acc5/val': top5.avg})\n",
    "\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "save_checkpoint({\n",
    "   'epoch': num_episodes,\n",
    "   'arch': 'DQN',\n",
    "   'state_dict': target_net.state_dict(),\n",
    "   'optimizer' : optimizer.state_dict(),\n",
    "    })\n",
    "\n",
    "print('Complete')\n",
    "# env.render()\n",
    "# env.close()\n",
    "# plt.ioff()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING CODE BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119415"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions = memory.sample(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = Transition(*zip(*transitions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(non_final_mask), non_final_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_final_next_states[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_batch[reward_batch[:]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_action_values = policy_net(state_batch).gather(1, action_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "len(next_state_values), next_state_values[100:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "expected_state_action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.SmoothL1Loss()\n",
    "loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "acc1, acc5 = accuracy(state_action_values,  expected_state_action_values.unsqueeze(1), topk=(1, 5)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc1, acc5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_chess_pos = [[1.,  0.,  0.,  0.,  0.,  0.,  0.,  8.,  0., 0., 0., 0., 0.,  0.,\n",
    "        0., 0.,  0.,  0.,  0.,  0.,  0., 0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0., 0.,  0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_pos = torch.tensor(test_chess_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_pos = target_net(tensor_pos).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3066,  0.2115, -0.4388, -0.1213,  0.0079, -0.1712,  0.0113,  0.6572,\n",
       "         -0.1239,  0.0122,  0.1557,  0.4796, -0.1617,  0.0660,  0.0131,  0.0111,\n",
       "          0.1753,  0.0114, -0.0105, -0.2700,  0.0644,  0.2545,  0.5639,  0.1263,\n",
       "         -0.0946, -0.0070, -0.0598, -0.1888,  0.0090,  0.2693,  0.0326, -0.2249,\n",
       "         -0.0916,  0.0275,  0.1161,  2.0285, -0.2500,  0.1548,  0.0123,  0.3809,\n",
       "          0.2924, -0.0916,  0.0192,  0.0128, -0.0212, -0.2283,  0.0116, -0.3731,\n",
       "         -0.3400,  2.0707,  0.3080,  0.4379,  0.0141,  0.0080,  0.1105,  0.0111]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(49, device='cuda:0')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = target_net(tensor_pos).argmax(1)[0].detach()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 3, 1]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_action = env.complete_actions_list[x]\n",
    "next_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 8., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0.]]), 1, True, [])\n"
     ]
    }
   ],
   "source": [
    "print(env.step(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
