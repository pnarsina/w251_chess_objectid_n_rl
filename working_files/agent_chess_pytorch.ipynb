{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from gym_chess_env import ChessBoard_gym\n",
    "import wandb\n",
    "from onecyclelr import OneCycleLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: pnarsina (use `wandb login --relogin` to force relogin)\n",
      "wandb: wandb version 0.11.1 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.32<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">deft-serenity-41</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/pnarsina/w251-prabhu-final_chessproject-notebook\" target=\"_blank\">https://wandb.ai/pnarsina/w251-prabhu-final_chessproject-notebook</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/pnarsina/w251-prabhu-final_chessproject-notebook/runs/1ta6ju4h\" target=\"_blank\">https://wandb.ai/pnarsina/w251-prabhu-final_chessproject-notebook/runs/1ta6ju4h</a><br/>\n",
       "                Run data is saved locally in <code>C:\\prabhu\\edu\\code\\w251\\myrepo\\chess_project\\wandb\\run-20210801_203511-1ta6ju4h</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(1ta6ju4h)</h1><iframe src=\"https://wandb.ai/pnarsina/w251-prabhu-final_chessproject-notebook/runs/1ta6ju4h\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1b5acd391f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()\n",
    "wandb.init(project=\"w251-prabhu-final_chessproject-notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ChessBoard_gym()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.DataParallel(nn.Linear(64, 128))\n",
    "        self.fc2 = nn.DataParallel(nn.Linear(128, 512))\n",
    "        self.fc3 = nn.DataParallel(nn.Linear(512, 256))\n",
    "        self.fc4 = nn.DataParallel(nn.Linear(256, 128))\n",
    "#         self.bn1 = nn.BatchNorm1d(128)\n",
    "#         self.conv1 = nn.Conv1d(16, 16, kernel_size=3, stride=1)\n",
    "#         self.bn1 = nn.BatchNorm1d(16)\n",
    "#         self.conv2 = nn.Conv1d(16, 32, kernel_size=5, stride=2)\n",
    "#         self.bn2 = nn.BatchNorm1d(32)\n",
    "#         self.conv3 = nn.Conv1d(32, 16, kernel_size=5, stride=2)\n",
    "#         self.bn3 = nn.BatchNorm1d(16)\n",
    "\n",
    "#         # Number of Linear input connections depends on output of conv2d layers\n",
    "#         # and therefore the input image size, so compute it.\n",
    "#         def conv2d_size_out(size, kernel_size = 5, stride = ):\n",
    "#             return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        \n",
    "#         def conv1d_size_out(size, kernel_size = 5, stride = 2):\n",
    "#             return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "\n",
    "#         convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "#         convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "#         linear_input_size = convw * convh * 32\n",
    "        self.head = nn.DataParallel(nn.Linear(128, outputs))\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "#     def forward(self, x):\n",
    "#         x = x.to(device)\n",
    "#         x = F.relu(self.conv1(x))\n",
    "# #         x = F.relu(self.bn1(self.conv1(x)))\n",
    "# #         x = F.relu(self.bn2(self.conv2(x)))\n",
    "# #         x = F.relu(self.bn3(self.conv3(x)))\n",
    "#         return self.head(x.view(x.size(0), -1))\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "br1 - BL - BL - BL - BL - BL - BL - br2 -  \n",
      "\n",
      "BL - BL - BL - BL - BL - BL - BL - BL -  \n",
      "\n",
      "BL - BL - BL - BL - BL - BL - BL - BL -  \n",
      "\n",
      "BL - BL - BL - BL - BL - BL - BL - BL -  \n",
      "\n",
      "BL - BL - BL - BL - BL - BL - BL - BL -  \n",
      "\n",
      "BL - BL - BL - BL - BL - BL - BL - BL -  \n",
      "\n",
      "BL - BL - BL - BL - BL - BL - BL - BL -  \n",
      "\n",
      "wr1 - BL - BL - BL - BL - BL - BL - wr2 -  \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.print_board()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chess Cells 8 x 8\n",
    "height = 8 \n",
    "width = 8\n",
    "n_actions = env.action_space.n\n",
    "# n_actions = 112\n",
    "BATCH_SIZE = 2048\n",
    "GAMMA = 0.999\n",
    "EPS_START = 1 \n",
    "EPS_END = 0.10\n",
    "EPS_DECAY = 800000\n",
    "TARGET_UPDATE = 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = DQN(height, width, n_actions).to(device)\n",
    "target_net = DQN(height, width, n_actions).to(device)\n",
    "# policy_net = DQN(n_actions).to(device)\n",
    "# target_net = DQN(n_actions).to(device)\n",
    "\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "# optimizer = optim.RMSprop(policy_net.parameters())\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100)\n",
    "# optimizer = optim.AdamW(policy_net.parameters())\n",
    "# optimizer = optim.SGD(lr=0.0001 )\n",
    "memory = ReplayMemory(500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    global steps_done\n",
    "    \n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    \n",
    "#     print(\"epsilon threshold:\", eps_threshold)\n",
    "    \n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "#         plt.plot(means.numpy())\n",
    "\n",
    "#     plt.pause(0.5)  # pause a bit so that plots are updated\n",
    "#     if is_ipython:\n",
    "#         display.clear_output(wait=True)\n",
    "#         display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "\n",
    "    pred = output \n",
    "    correct = pred.eq(target)\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k)\n",
    "    return res\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
    "\n",
    "# In[13]:\n",
    "def save_checkpoint(state,  filename='checkpoint.pth.tar'):\n",
    "    # save the model state!\n",
    "    torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return(0,0,0)\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "  \n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    acc1, acc5 = accuracy(state_action_values,  expected_state_action_values.unsqueeze(1), topk=(1, 5)) \n",
    "   # print('accuracies ', acc1, \" \", acc5)\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    return (loss,acc1,acc5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss of the episode  0  is : 0  acc1: 0 acc5: 0  Batch size: 2048  Memory size: 812 reward mean: -412.0\n",
      "loss of the episode  1  is : 0  acc1: 0 acc5: 0  Batch size: 2048  Memory size: 1483 reward mean: -341.5\n",
      "loss of the episode  2  is : tensor(31.4521, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 2585 reward mean: -461.6666666666667\n",
      "loss of the episode  3  is : tensor(1.6334, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 3352 reward mean: -438.0\n",
      "loss of the episode  4  is : tensor(1.2591, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 4251 reward mean: -450.2\n",
      "loss of the episode  5  is : tensor(1.2051, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 5055 reward mean: -442.5\n",
      "loss of the episode  6  is : tensor(1.2188, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 5891 reward mean: -441.57142857142856\n",
      "loss of the episode  7  is : tensor(0.8921, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 6667 reward mean: -433.375\n",
      "loss of the episode  8  is : tensor(0.9551, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 7407 reward mean: -423.0\n",
      "loss of the episode  9  is : tensor(0.8445, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 8128 reward mean: -412.8\n",
      "loss of the episode  10  is : tensor(0.7906, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 8906 reward mean: -409.6363636363636\n",
      "loss of the episode  11  is : tensor(21.2915, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 9784 reward mean: -415.3333333333333\n",
      "loss of the episode  12  is : tensor(7.7103, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 11715 reward mean: -501.15384615384613\n",
      "loss of the episode  13  is : tensor(2.1590, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 12449 reward mean: -489.2142857142857\n",
      "loss of the episode  14  is : tensor(1.8700, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 13325 reward mean: -488.3333333333333\n",
      "loss of the episode  15  is : tensor(1.6507, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 14130 reward mean: -483.125\n",
      "loss of the episode  16  is : tensor(1.6186, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 15027 reward mean: -483.94117647058823\n",
      "loss of the episode  17  is : tensor(1.3789, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 15847 reward mean: -480.3888888888889\n",
      "loss of the episode  18  is : tensor(3.5131, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 17356 reward mean: -513.4736842105264\n",
      "loss of the episode  19  is : tensor(1.8724, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 18212 reward mean: -510.6\n",
      "loss of the episode  20  is : tensor(2.9742, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 20162 reward mean: -560.0952380952381\n",
      "loss of the episode  21  is : tensor(33.7289, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 20912 reward mean: -550.5454545454545\n",
      "loss of the episode  22  is : tensor(7.9953, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 21703 reward mean: -543.6086956521739\n",
      "loss of the episode  23  is : tensor(5.1758, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 22532 reward mean: -538.8333333333334\n",
      "loss of the episode  24  is : tensor(3.0116, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 23243 reward mean: -529.72\n",
      "loss of the episode  25  is : tensor(2.7924, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 24043 reward mean: -524.7307692307693\n",
      "loss of the episode  26  is : tensor(2.1365, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 24787 reward mean: -518.0370370370371\n",
      "loss of the episode  27  is : tensor(2.1381, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 25545 reward mean: -512.3214285714286\n",
      "loss of the episode  28  is : tensor(2.2718, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 26305 reward mean: -507.0689655172414\n",
      "loss of the episode  29  is : tensor(3.1437, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 27355 reward mean: -511.8333333333333\n",
      "loss of the episode  30  is : tensor(2.2966, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 28058 reward mean: -505.0967741935484\n",
      "loss of the episode  31  is : tensor(36.9144, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 28754 reward mean: -498.5625\n",
      "loss of the episode  32  is : tensor(17.0089, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 29973 reward mean: -508.27272727272725\n",
      "loss of the episode  33  is : tensor(7.5152, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 30763 reward mean: -504.79411764705884\n",
      "loss of the episode  34  is : tensor(5.7168, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 31539 reward mean: -501.1142857142857\n",
      "loss of the episode  35  is : tensor(4.1326, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 32203 reward mean: -494.52777777777777\n",
      "loss of the episode  36  is : tensor(4.0614, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 32943 reward mean: -490.35135135135135\n",
      "loss of the episode  37  is : tensor(3.6691, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 33747 reward mean: -488.07894736842104\n",
      "loss of the episode  38  is : tensor(2.8671, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 34415 reward mean: -482.43589743589746\n",
      "loss of the episode  39  is : tensor(2.9147, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 35146 reward mean: -478.65\n",
      "loss of the episode  40  is : tensor(3.5495, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 36002 reward mean: -478.0975609756098\n",
      "loss of the episode  41  is : tensor(48.1329, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 36971 reward mean: -480.26190476190476\n",
      "loss of the episode  42  is : tensor(11.5922, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([1.], device='cuda:0')  Batch size: 2048  Memory size: 37709 reward mean: -476.95348837209303\n",
      "loss of the episode  43  is : tensor(8.6954, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 38400 reward mean: -472.72727272727275\n",
      "loss of the episode  44  is : tensor(8.5672, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 39276 reward mean: -472.8\n",
      "loss of the episode  45  is : tensor(6.3299, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 40026 reward mean: -470.1304347826087\n",
      "loss of the episode  46  is : tensor(4.8107, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 40679 reward mean: -465.51063829787233\n",
      "loss of the episode  47  is : tensor(10.6165, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 42183 reward mean: -478.8125\n",
      "loss of the episode  48  is : tensor(8.5917, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 43374 reward mean: -485.18367346938777\n",
      "loss of the episode  49  is : tensor(5.1047, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 44038 reward mean: -480.76\n",
      "loss of the episode  50  is : tensor(5.3974, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 44737 reward mean: -478.5\n",
      "loss of the episode  51  is : tensor(61.1744, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 45562 reward mean: -481.58\n",
      "loss of the episode  52  is : tensor(25.3722, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 46558 reward mean: -479.46\n",
      "loss of the episode  53  is : tensor(18.0014, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 47680 reward mean: -486.56\n",
      "loss of the episode  54  is : tensor(10.5754, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 48463 reward mean: -484.24\n",
      "loss of the episode  55  is : tensor(9.2020, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 49153 reward mean: -481.96\n",
      "loss of the episode  56  is : tensor(8.2158, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 49813 reward mean: -478.44\n",
      "loss of the episode  57  is : tensor(8.9056, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([1.], device='cuda:0')  Batch size: 2048  Memory size: 50596 reward mean: -478.58\n",
      "loss of the episode  58  is : tensor(7.6877, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 51314 reward mean: -478.14\n",
      "loss of the episode  59  is : tensor(13.7529, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 52968 reward mean: -496.8\n",
      "loss of the episode  60  is : tensor(4.9853, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 53620 reward mean: -494.28\n",
      "loss of the episode  61  is : tensor(91.1679, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 55122 reward mean: -506.76\n",
      "loss of the episode  62  is : tensor(17.4054, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 55901 reward mean: -483.72\n",
      "loss of the episode  63  is : tensor(12.8659, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 56539 reward mean: -481.8\n",
      "loss of the episode  64  is : tensor(12.0767, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 57184 reward mean: -477.18\n",
      "loss of the episode  65  is : tensor(14.1140, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 58045 reward mean: -478.3\n",
      "loss of the episode  66  is : tensor(10.8558, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 58757 reward mean: -474.6\n",
      "loss of the episode  67  is : tensor(12.4442, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 59543 reward mean: -473.92\n",
      "loss of the episode  68  is : tensor(11.3597, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 60246 reward mean: -457.8\n",
      "loss of the episode  69  is : tensor(10.6935, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 60931 reward mean: -454.38\n",
      "loss of the episode  70  is : tensor(11.3663, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 61673 reward mean: -430.22\n",
      "loss of the episode  71  is : tensor(80.9489, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 62348 reward mean: -428.72\n",
      "loss of the episode  72  is : tensor(54.4607, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 63518 reward mean: -436.3\n",
      "loss of the episode  73  is : tensor(39.8409, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 64951 reward mean: -448.38\n",
      "loss of the episode  74  is : tensor(21.2657, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 65985 reward mean: -454.84\n",
      "loss of the episode  75  is : tensor(15.0517, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 66759 reward mean: -454.32\n",
      "loss of the episode  76  is : tensor(12.8660, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 67414 reward mean: -452.54\n",
      "loss of the episode  77  is : tensor(14.9027, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 68136 reward mean: -451.82\n",
      "loss of the episode  78  is : tensor(13.6611, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 68805 reward mean: -450.0\n",
      "loss of the episode  79  is : tensor(14.5800, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([1.], device='cuda:0') acc5: tensor([1.], device='cuda:0')  Batch size: 2048  Memory size: 69495 reward mean: -442.8\n",
      "loss of the episode  80  is : tensor(19.6532, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 70433 reward mean: -447.5\n",
      "loss of the episode  81  is : tensor(96.5132, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 71102 reward mean: -446.96\n",
      "loss of the episode  82  is : tensor(45.2504, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 71784 reward mean: -436.22\n",
      "loss of the episode  83  is : tensor(35.3460, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 72441 reward mean: -433.56\n",
      "loss of the episode  84  is : tensor(32.4302, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 73134 reward mean: -431.9\n",
      "loss of the episode  85  is : tensor(38.6425, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 74112 reward mean: -438.18\n",
      "loss of the episode  86  is : tensor(23.4550, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 74783 reward mean: -436.8\n",
      "loss of the episode  87  is : tensor(24.0292, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 75489 reward mean: -434.84\n",
      "loss of the episode  88  is : tensor(23.0706, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 76182 reward mean: -435.34\n",
      "loss of the episode  89  is : tensor(20.8406, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 76824 reward mean: -433.56\n",
      "loss of the episode  90  is : tensor(28.7148, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 77740 reward mean: -434.76\n",
      "loss of the episode  91  is : tensor(124.6992, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 78488 reward mean: -430.34\n",
      "loss of the episode  92  is : tensor(53.3940, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 79121 reward mean: -428.24\n",
      "loss of the episode  93  is : tensor(46.4522, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 79773 reward mean: -427.46\n",
      "loss of the episode  94  is : tensor(43.7556, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 80456 reward mean: -423.6\n",
      "loss of the episode  95  is : tensor(57.4610, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 81462 reward mean: -428.72\n",
      "loss of the episode  96  is : tensor(33.5773, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 82115 reward mean: -428.72\n",
      "loss of the episode  97  is : tensor(36.9465, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 82860 reward mean: -413.54\n",
      "loss of the episode  98  is : tensor(35.2627, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 83585 reward mean: -404.22\n",
      "loss of the episode  99  is : tensor(71.3028, device='cuda:0', grad_fn=<AddBackward0>)  acc1: tensor([0.], device='cuda:0') acc5: tensor([0.], device='cuda:0')  Batch size: 2048  Memory size: 85141 reward mean: -422.06\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 100\n",
    "steps_done = 0\n",
    "observation_space = 64\n",
    "episode_durations = []\n",
    "rewards_list = []\n",
    "for i_episode in range(num_episodes):\n",
    "    reward_for_episode = 0\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    data_time = AverageMeter('Data', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    progress = ProgressMeter(\n",
    "        len(memory),\n",
    "        [batch_time, data_time, losses, top1, top5],\n",
    "        prefix=\"Epoch: [{}]\".format(i_episode))\n",
    "\n",
    "    # Initialize the environment and state\n",
    "    state = torch.from_numpy(env.reset()).float()\n",
    "    total_loss = 0\n",
    "    loss = 0\n",
    "    t=0\n",
    "    scheduler.step()\n",
    "    end = time.time()\n",
    "    legal_move_count=0\n",
    "    tot_acc1 = 0\n",
    "    tot_acc5 = 0\n",
    "    while True:\n",
    "        # Select and perform an action\n",
    "#         state_model_input = np.reshape(state, state.shape + (1,)) \n",
    "        state_model_input = torch.reshape(state, [1, observation_space])\n",
    "        \n",
    "        action = select_action(state_model_input)\n",
    "        next_state, reward, done, info = env.step(action.item())\n",
    "        \n",
    "        reward_for_episode += reward\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        \n",
    "       \n",
    "        # Store the transition in memory\n",
    "        next_state = torch.from_numpy(next_state).float()\n",
    "        \n",
    "#         next_state_model_input = np.reshape(next_state, next_state.shape + (1,)) \n",
    "        next_state_model_input = torch.reshape(next_state, [1, observation_space])\n",
    "    \n",
    "        memory.push(state_model_input, action, next_state_model_input, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        loss, acc1, acc5 = optimize_model()\n",
    "\n",
    "\n",
    "        if loss is not None: \n",
    "            total_loss = total_loss + loss\n",
    "            tot_acc1 = tot_acc1 + acc1\n",
    "            tot_acc5 = tot_acc5 + acc5\n",
    "\n",
    "            # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "        if done:\n",
    "            legal_move_count += 1\n",
    "\n",
    "        if legal_move_count >=200:\n",
    "            episode_durations.append(t + 1)\n",
    "            t+=1\n",
    "\n",
    "            if loss is not None: \n",
    "                total_loss = total_loss + loss\n",
    "                losses.update(total_loss, BATCH_SIZE)\n",
    "                top1.update(tot_acc1,  BATCH_SIZE)\n",
    "                top5.update(tot_acc5, BATCH_SIZE)\n",
    "            #plot_durations()\n",
    "            break\n",
    "            \n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    rewards_list.append(reward_for_episode)\n",
    "    last_rewards_mean = np.mean(rewards_list[-50:])\n",
    "    print (\"loss of the episode \", i_episode , \" is :\", total_loss, \" acc1:\", tot_acc1, \"acc5:\", tot_acc5, \" Batch size:\",BATCH_SIZE , \" Memory size:\", len(memory), \"reward mean:\", last_rewards_mean)\n",
    "    wandb.log({\"Loss/val\": losses.avg, 'acc1/val': top1.avg, 'acc5/val': top5.avg})\n",
    "\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "save_checkpoint({\n",
    "   'epoch': num_episodes,\n",
    "   'arch': 'DQN',\n",
    "   'state_dict': target_net.state_dict(),\n",
    "   'optimizer' : optimizer.state_dict(),\n",
    "    })\n",
    "\n",
    "print('Complete')\n",
    "# env.render()\n",
    "# env.close()\n",
    "# plt.ioff()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onecyclelr import OneCycleLR\n",
    "\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "scheduler = OneCycleLR(optimizer, num_steps=num_steps, lr_range=(0.1, 1.))\n",
    "for epoch in range(epochs):\n",
    "    for step, X in enumerate(train_dataloader):\n",
    "        train(...) \n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING CODE BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119415"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions = memory.sample(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = Transition(*zip(*transitions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(non_final_mask), non_final_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_final_next_states[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_batch[reward_batch[:]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_action_values = policy_net(state_batch).gather(1, action_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "len(next_state_values), next_state_values[100:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "expected_state_action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.SmoothL1Loss()\n",
    "loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "acc1, acc5 = accuracy(state_action_values,  expected_state_action_values.unsqueeze(1), topk=(1, 5)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc1, acc5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_chess_pos = [[1.,  0.,  0.,  0.,  0.,  0.,  0.,  8.,  0., 0., 0., 0., 0.,  0.,\n",
    "        0., 0.,  0.,  0.,  0.,  0.,  0., 0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0., 0.,  0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_pos = torch.tensor(test_chess_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_pos = target_net(tensor_pos).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3066,  0.2115, -0.4388, -0.1213,  0.0079, -0.1712,  0.0113,  0.6572,\n",
       "         -0.1239,  0.0122,  0.1557,  0.4796, -0.1617,  0.0660,  0.0131,  0.0111,\n",
       "          0.1753,  0.0114, -0.0105, -0.2700,  0.0644,  0.2545,  0.5639,  0.1263,\n",
       "         -0.0946, -0.0070, -0.0598, -0.1888,  0.0090,  0.2693,  0.0326, -0.2249,\n",
       "         -0.0916,  0.0275,  0.1161,  2.0285, -0.2500,  0.1548,  0.0123,  0.3809,\n",
       "          0.2924, -0.0916,  0.0192,  0.0128, -0.0212, -0.2283,  0.0116, -0.3731,\n",
       "         -0.3400,  2.0707,  0.3080,  0.4379,  0.0141,  0.0080,  0.1105,  0.0111]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(49, device='cuda:0')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = target_net(tensor_pos).argmax(1)[0].detach()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 3, 1]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_action = env.complete_actions_list[x]\n",
    "next_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 8., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0.]]), 1, True, [])\n"
     ]
    }
   ],
   "source": [
    "print(env.step(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
