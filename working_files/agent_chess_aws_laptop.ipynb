{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, math\n",
    "import numpy as np\n",
    "from gym_chess_env import ChessBoard_gym\n",
    "import Box2D\n",
    "from Box2D.b2 import (edgeShape, circleShape, fixtureDef, polygonShape, revoluteJointDef, contactListener)\n",
    "# new line\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "# import skvideo.io\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,BatchNormalization\n",
    "from collections import deque\n",
    "from tensorflow.keras.activations import relu, linear\n",
    "from tensorflow.keras.losses import mean_squared_error\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "import random\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, env):\n",
    "\n",
    "        self.env = env\n",
    "        self.action_space = env.action_space\n",
    "        self.observation_space = env.observation_space\n",
    "        self.counter = 0\n",
    "\n",
    "        #######################\n",
    "        # Change these parameters to improve performance\n",
    "        self.density_first_layer = 512\n",
    "        self.density_second_layer = 512\n",
    "        self.num_epochs = 1\n",
    "        self.batch_size = 64\n",
    "        self.epsilon_min = 0.01\n",
    "\n",
    "        # epsilon will randomly choose the next action as either\n",
    "        # a random action, or the highest scoring predicted action\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.gamma = 0.995\n",
    "\n",
    "        # Learning rate\n",
    "        self.lr = 0.003\n",
    "        self.rewards_list = []\n",
    "\n",
    "        self.replay_memory_buffer = deque(maxlen=500000)\n",
    "        self.num_action_space = self.action_space.n\n",
    "#         self.num_observation_space = env.observation_space.shape[0]\n",
    "# HARD CODED FOR NOW\n",
    "        self.num_observation_space = 64\n",
    "\n",
    "        self.model = self.initialize_model()\n",
    "\n",
    "    def initialize_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(self.density_first_layer, input_dim=self.num_observation_space, activation=relu))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(self.density_second_layer, activation=relu))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(self.num_action_space, activation=relu))\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(loss=mean_squared_error,optimizer=SGD(lr=self.lr))\n",
    "        print(model.summary())\n",
    "        return model\n",
    "\n",
    "    def get_action(self, state):\n",
    "\n",
    "        # The epsilon parameter decides whether we are using the \n",
    "        # Q-function to determine our next action \n",
    "        # or take a random sample of the action space. \n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return random.randrange(self.num_action_space)\n",
    "\n",
    "        # Get a list of predictions based on the current state\n",
    "        predicted_actions = self.model.predict(state)\n",
    "\n",
    "        # Return the maximum-reward action\n",
    "        return np.argmax(predicted_actions[0])\n",
    "\n",
    "    def add_to_replay_memory(self, state, action, reward, next_state, done):\n",
    "        self.replay_memory_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def learn_and_update_weights_by_reply(self):\n",
    "\n",
    "        # replay_memory_buffer size check\n",
    "        # if we have fewer than 64 actions in the buffer, \n",
    "        # or the counter is not 0, return\n",
    "        if len(self.replay_memory_buffer) < self.batch_size or self.counter != 0:\n",
    "            return\n",
    "\n",
    "        # Early Stopping\n",
    "        if np.mean(self.rewards_list[-10:]) > 180:\n",
    "            return\n",
    "\n",
    "        # Choose batch of random samples from the replay stack \n",
    "        random_sample = self.get_random_sample_from_replay_mem()\n",
    "\n",
    "        # Get the values (in numpy array form) from the random batch of samples\n",
    "        states, actions, rewards, next_states, done_list = self.get_attribues_from_sample(random_sample)\n",
    "\n",
    "        # Use the Keras \"predict_on_batch\" feature to predict the targets\n",
    "        # based on the random batch of next states in our replay stack\n",
    "#         print('next states: \\n ', next_states, '1- done_list: \\n', 1- done_list)\n",
    "#         pred = self.model.predict_on_batch(next_states)\n",
    "#         print( \"shape of pred is: \", pred.shape )\n",
    "#         print ('prediction')\n",
    "#         print (pred)\n",
    "#         print('amax')\n",
    "#         print( np.amax(self.model.predict_on_batch(next_states), axis=1) )        \n",
    "        targets = rewards + self.gamma * (np.amax(self.model.predict_on_batch(next_states), axis=1)) * (1 - done_list)\n",
    "        \n",
    "        # Run a prediction on the states in our random sample\n",
    "        target_vec = self.model.predict_on_batch(states)\n",
    "\n",
    "        # Create a numpy array sized to match the batch_size\n",
    "        indexes = np.array([i for i in range(self.batch_size)])\n",
    "\n",
    "        # The target vector is an array of \n",
    "        # state predictions \n",
    "        target_vec[[indexes], [actions]] = targets\n",
    "\n",
    "        # build a model with the existing states and target scores in batches of 64\n",
    "#         print('shape of target_vec', target_vec.shape,' shape of state:', states.shape ,'\\ntarget_vectors:', target_vec )\n",
    "        self.model.fit(states, target_vec, epochs=self.num_epochs, verbose=0)\n",
    "\n",
    "    def get_attribues_from_sample(self, random_sample):\n",
    "        states = np.array([i[0] for i in random_sample])\n",
    "        actions = np.array([i[1] for i in random_sample])\n",
    "        rewards = np.array([i[2] for i in random_sample])\n",
    "        next_states = np.array([i[3] for i in random_sample])\n",
    "        done_list = np.array([i[4] for i in random_sample])\n",
    "        states = np.squeeze(states)\n",
    "        next_states = np.squeeze(next_states)\n",
    "        return np.squeeze(states), actions, rewards, next_states, done_list\n",
    "\n",
    "    # Get a batch_size sample of previous iterations\n",
    "    def get_random_sample_from_replay_mem(self):\n",
    "        random_sample = random.sample(self.replay_memory_buffer, self.batch_size)\n",
    "        return random_sample\n",
    "\n",
    "    # Run the keras predict using the current state as input.\n",
    "    # This will choose the next step.\n",
    "    def predict(self, current_state):\n",
    "        return self.model.predict(current_state)\n",
    "\n",
    "    def train(self, num_episodes=2000, can_stop=True):\n",
    "\n",
    "        frames = []\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "\n",
    "            # state is a vector of 8 values:\n",
    "            # x and y position\n",
    "            # x and y velocity\n",
    "            # lander angle and angular velocity\n",
    "            # boolean for left leg contact with ground\n",
    "            # boolean for right leg contact with ground\n",
    "            state = env.reset()\n",
    "            reward_for_episode = 0\n",
    "            done = False\n",
    "            state = np.reshape(state, [1, self.num_observation_space])\n",
    "                \n",
    "#             if episode > 0: state = env.reset()\n",
    "            ctr = 0\n",
    "            while not done:\n",
    "\n",
    "                if episode % 50 == 0:\n",
    "                    frame = env.get_FEN()\n",
    "                    if ctr==100: env.print_board()\n",
    "\n",
    "                if episode % 50 == 0:\n",
    "                    if ctr % 200 == 0: print(\"rewards earned so far is: \", reward_for_episode)\n",
    "                    frames.append(frame)      \n",
    "\n",
    "                # use epsilon decay to choose the next state\n",
    "                received_action = self.get_action(state)\n",
    "                next_state, reward, done, info = env.step(received_action)\n",
    "\n",
    "                # Reshape the next_state array to match the size of the observation space\n",
    "                next_state = np.reshape(next_state, [1, self.num_observation_space])\n",
    "\n",
    "                # Store the experience in replay memory\n",
    "                self.add_to_replay_memory(state, received_action, reward, next_state, done)\n",
    "\n",
    "                # add up rewards\n",
    "                reward_for_episode += reward\n",
    "                state = next_state\n",
    "                self.update_counter()\n",
    "\n",
    "                # update the model\n",
    "                self.learn_and_update_weights_by_reply()\n",
    "                ctr += 1\n",
    "                \n",
    "                #if done:\n",
    "                #    break\n",
    "            self.rewards_list.append(reward_for_episode)\n",
    "\n",
    "            # Create a video from every 10th episode\n",
    "#             if episode % 50 == 0:\n",
    "#                 fname = \"/tmp/videos/episode\"+str(episode)+\".mp4\"\n",
    "#                 skvideo.io.vwrite(fname, np.array(frames))\n",
    "#                 del frames\n",
    "#                 frames = []\n",
    "\n",
    "            # Decay the epsilon after each experience completion\n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon *= self.epsilon_decay\n",
    "                #self.epsilon *= min(0.995,(self.epsilon_decay + counter*(0.000075)))\n",
    "\n",
    "            # Check for breaking condition\n",
    "            last_rewards_mean = np.mean(self.rewards_list[-100:])\n",
    "\n",
    "            # Once the mean average of rewards is over 200, we can stop training\n",
    "            if last_rewards_mean > 200 and can_stop:\n",
    "                print(\"DQN Training Complete...\")\n",
    "                break\n",
    "            print(episode, \"\\t: Episode || Reward: \",reward_for_episode, \"\\t|| Average Reward: \",last_rewards_mean, \"\\t epsilon: \", self.epsilon )\n",
    "\n",
    "    def update_counter(self):\n",
    "        self.counter += 1\n",
    "        step_size = 5\n",
    "        self.counter = self.counter % step_size\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 512)               33280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 13)                6669      \n",
      "=================================================================\n",
      "Total params: 306,701\n",
      "Trainable params: 304,653\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n",
      "None\n",
      "rewards earned so far is:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3334: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "BL - BL - BL - BL - BL - BL - BL - BL -  \n",
      "\n",
      "bp1 - BL - BL - bp4 - bp5 - bp6 - bp7 - bp8 -  \n",
      "\n",
      "BL - bp2 - bp3 - BL - BL - BL - BL - BL -  \n",
      "\n",
      "BL - BL - BL - BL - BL - BL - BL - BL -  \n",
      "\n",
      "BL - BL - BL - BL - BL - BL - BL - BL -  \n",
      "\n",
      "BL - BL - BL - wp4 - BL - wp6 - BL - BL -  \n",
      "\n",
      "wp1 - wp2 - wp3 - BL - wp5 - BL - wp7 - wp8 -  \n",
      "\n",
      "BL - BL - BL - BL - BL - BL - BL - BL -  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rewards earned so far is:  -172\n",
      "rewards earned so far is:  -322\n",
      "episode completed, board position is\n",
      "0 \t: Episode || Reward:  -396 \t|| Average Reward:  -396.0 \t epsilon:  0.999\n",
      "episode completed, board position is\n",
      "1 \t: Episode || Reward:  -396 \t|| Average Reward:  -396.0 \t epsilon:  0.998001\n",
      "episode completed, board position is\n",
      "2 \t: Episode || Reward:  -414 \t|| Average Reward:  -402.0 \t epsilon:  0.997002999\n",
      "episode completed, board position is\n",
      "3 \t: Episode || Reward:  -416 \t|| Average Reward:  -405.5 \t epsilon:  0.996005996001\n",
      "episode completed, board position is\n",
      "4 \t: Episode || Reward:  -406 \t|| Average Reward:  -405.6 \t epsilon:  0.995009990004999\n",
      "episode completed, board position is\n",
      "5 \t: Episode || Reward:  -430 \t|| Average Reward:  -409.6666666666667 \t epsilon:  0.994014980014994\n",
      "episode completed, board position is\n",
      "6 \t: Episode || Reward:  -428 \t|| Average Reward:  -412.2857142857143 \t epsilon:  0.993020965034979\n",
      "episode completed, board position is\n",
      "7 \t: Episode || Reward:  -404 \t|| Average Reward:  -411.25 \t epsilon:  0.9920279440699441\n",
      "episode completed, board position is\n",
      "8 \t: Episode || Reward:  -396 \t|| Average Reward:  -409.55555555555554 \t epsilon:  0.9910359161258742\n",
      "episode completed, board position is\n",
      "9 \t: Episode || Reward:  -370 \t|| Average Reward:  -405.6 \t epsilon:  0.9900448802097482\n",
      "episode completed, board position is\n",
      "10 \t: Episode || Reward:  -390 \t|| Average Reward:  -404.1818181818182 \t epsilon:  0.9890548353295385\n",
      "episode completed, board position is\n",
      "11 \t: Episode || Reward:  -396 \t|| Average Reward:  -403.5 \t epsilon:  0.988065780494209\n",
      "episode completed, board position is\n",
      "12 \t: Episode || Reward:  -412 \t|| Average Reward:  -404.15384615384613 \t epsilon:  0.9870777147137147\n",
      "episode completed, board position is\n",
      "13 \t: Episode || Reward:  -380 \t|| Average Reward:  -402.42857142857144 \t epsilon:  0.986090636999001\n",
      "episode completed, board position is\n",
      "14 \t: Episode || Reward:  -392 \t|| Average Reward:  -401.73333333333335 \t epsilon:  0.9851045463620021\n",
      "episode completed, board position is\n",
      "15 \t: Episode || Reward:  -412 \t|| Average Reward:  -402.375 \t epsilon:  0.98411944181564\n",
      "episode completed, board position is\n",
      "16 \t: Episode || Reward:  -398 \t|| Average Reward:  -402.11764705882354 \t epsilon:  0.9831353223738244\n",
      "episode completed, board position is\n",
      "17 \t: Episode || Reward:  -420 \t|| Average Reward:  -403.1111111111111 \t epsilon:  0.9821521870514506\n",
      "episode completed, board position is\n",
      "18 \t: Episode || Reward:  -382 \t|| Average Reward:  -402.0 \t epsilon:  0.9811700348643991\n",
      "episode completed, board position is\n",
      "19 \t: Episode || Reward:  -418 \t|| Average Reward:  -402.8 \t epsilon:  0.9801888648295347\n",
      "episode completed, board position is\n",
      "20 \t: Episode || Reward:  -408 \t|| Average Reward:  -403.04761904761904 \t epsilon:  0.9792086759647052\n",
      "episode completed, board position is\n",
      "21 \t: Episode || Reward:  -438 \t|| Average Reward:  -404.6363636363636 \t epsilon:  0.9782294672887405\n",
      "episode completed, board position is\n",
      "22 \t: Episode || Reward:  -398 \t|| Average Reward:  -404.3478260869565 \t epsilon:  0.9772512378214517\n",
      "episode completed, board position is\n",
      "23 \t: Episode || Reward:  -400 \t|| Average Reward:  -404.1666666666667 \t epsilon:  0.9762739865836303\n",
      "episode completed, board position is\n",
      "24 \t: Episode || Reward:  -382 \t|| Average Reward:  -403.28 \t epsilon:  0.9752977125970467\n",
      "episode completed, board position is\n",
      "25 \t: Episode || Reward:  -362 \t|| Average Reward:  -401.6923076923077 \t epsilon:  0.9743224148844496\n",
      "episode completed, board position is\n",
      "26 \t: Episode || Reward:  -392 \t|| Average Reward:  -401.3333333333333 \t epsilon:  0.9733480924695651\n",
      "episode completed, board position is\n",
      "27 \t: Episode || Reward:  -434 \t|| Average Reward:  -402.5 \t epsilon:  0.9723747443770956\n",
      "episode completed, board position is\n",
      "28 \t: Episode || Reward:  -434 \t|| Average Reward:  -403.58620689655174 \t epsilon:  0.9714023696327185\n",
      "episode completed, board position is\n",
      "29 \t: Episode || Reward:  -380 \t|| Average Reward:  -402.8 \t epsilon:  0.9704309672630859\n",
      "episode completed, board position is\n",
      "30 \t: Episode || Reward:  -408 \t|| Average Reward:  -402.96774193548384 \t epsilon:  0.9694605362958227\n",
      "episode completed, board position is\n",
      "31 \t: Episode || Reward:  -370 \t|| Average Reward:  -401.9375 \t epsilon:  0.9684910757595269\n",
      "episode completed, board position is\n",
      "32 \t: Episode || Reward:  -402 \t|| Average Reward:  -401.93939393939394 \t epsilon:  0.9675225846837673\n",
      "episode completed, board position is\n",
      "33 \t: Episode || Reward:  -394 \t|| Average Reward:  -401.70588235294116 \t epsilon:  0.9665550620990835\n",
      "episode completed, board position is\n",
      "34 \t: Episode || Reward:  -416 \t|| Average Reward:  -402.1142857142857 \t epsilon:  0.9655885070369844\n",
      "episode completed, board position is\n",
      "35 \t: Episode || Reward:  -414 \t|| Average Reward:  -402.44444444444446 \t epsilon:  0.9646229185299474\n",
      "episode completed, board position is\n",
      "36 \t: Episode || Reward:  -384 \t|| Average Reward:  -401.94594594594594 \t epsilon:  0.9636582956114175\n",
      "episode completed, board position is\n",
      "37 \t: Episode || Reward:  -398 \t|| Average Reward:  -401.8421052631579 \t epsilon:  0.9626946373158061\n",
      "episode completed, board position is\n",
      "38 \t: Episode || Reward:  -414 \t|| Average Reward:  -402.15384615384613 \t epsilon:  0.9617319426784903\n",
      "episode completed, board position is\n",
      "39 \t: Episode || Reward:  -396 \t|| Average Reward:  -402.0 \t epsilon:  0.9607702107358118\n",
      "episode completed, board position is\n",
      "40 \t: Episode || Reward:  -384 \t|| Average Reward:  -401.5609756097561 \t epsilon:  0.959809440525076\n",
      "episode completed, board position is\n",
      "41 \t: Episode || Reward:  -432 \t|| Average Reward:  -402.2857142857143 \t epsilon:  0.9588496310845509\n",
      "episode completed, board position is\n",
      "42 \t: Episode || Reward:  -412 \t|| Average Reward:  -402.51162790697674 \t epsilon:  0.9578907814534664\n",
      "episode completed, board position is\n",
      "43 \t: Episode || Reward:  -378 \t|| Average Reward:  -401.95454545454544 \t epsilon:  0.9569328906720129\n",
      "episode completed, board position is\n",
      "44 \t: Episode || Reward:  -410 \t|| Average Reward:  -402.1333333333333 \t epsilon:  0.9559759577813409\n",
      "episode completed, board position is\n",
      "45 \t: Episode || Reward:  -436 \t|| Average Reward:  -402.8695652173913 \t epsilon:  0.9550199818235596\n",
      "episode completed, board position is\n",
      "46 \t: Episode || Reward:  -414 \t|| Average Reward:  -403.1063829787234 \t epsilon:  0.9540649618417361\n",
      "episode completed, board position is\n",
      "47 \t: Episode || Reward:  -426 \t|| Average Reward:  -403.5833333333333 \t epsilon:  0.9531108968798944\n",
      "episode completed, board position is\n",
      "48 \t: Episode || Reward:  -418 \t|| Average Reward:  -403.8775510204082 \t epsilon:  0.9521577859830145\n",
      "episode completed, board position is\n",
      "49 \t: Episode || Reward:  -406 \t|| Average Reward:  -403.92 \t epsilon:  0.9512056281970315\n",
      "rewards earned so far is:  0\n",
      "\n",
      "\n",
      "\n",
      "BL - BL - BL - BL - BL - BL - BL - BL -  \n",
      "\n",
      "bp1 - bp2 - bp3 - bp4 - BL - bp6 - BL - bp8 -  \n",
      "\n",
      "BL - BL - BL - BL - bp5 - BL - bp7 - BL -  \n",
      "\n",
      "BL - BL - BL - BL - BL - BL - BL - BL -  \n",
      "\n",
      "wp1 - BL - BL - BL - BL - BL - BL - BL -  \n",
      "\n",
      "BL - BL - BL - wp4 - BL - BL - BL - BL -  \n",
      "\n",
      "BL - wp2 - wp3 - BL - wp5 - wp6 - wp7 - wp8 -  \n",
      "\n",
      "BL - BL - BL - BL - BL - BL - BL - BL -  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rewards earned so far is:  -176\n",
      "rewards earned so far is:  -348\n",
      "episode completed, board position is\n",
      "50 \t: Episode || Reward:  -434 \t|| Average Reward:  -404.5098039215686 \t epsilon:  0.9502544225688344\n",
      "episode completed, board position is\n",
      "51 \t: Episode || Reward:  -434 \t|| Average Reward:  -405.0769230769231 \t epsilon:  0.9493041681462656\n",
      "episode completed, board position is\n",
      "52 \t: Episode || Reward:  -438 \t|| Average Reward:  -405.6981132075472 \t epsilon:  0.9483548639781193\n",
      "episode completed, board position is\n",
      "53 \t: Episode || Reward:  -400 \t|| Average Reward:  -405.5925925925926 \t epsilon:  0.9474065091141411\n",
      "episode completed, board position is\n",
      "54 \t: Episode || Reward:  -404 \t|| Average Reward:  -405.56363636363636 \t epsilon:  0.946459102605027\n",
      "episode completed, board position is\n",
      "55 \t: Episode || Reward:  -354 \t|| Average Reward:  -404.64285714285717 \t epsilon:  0.9455126435024219\n",
      "episode completed, board position is\n",
      "56 \t: Episode || Reward:  -420 \t|| Average Reward:  -404.9122807017544 \t epsilon:  0.9445671308589195\n",
      "episode completed, board position is\n",
      "57 \t: Episode || Reward:  -412 \t|| Average Reward:  -405.0344827586207 \t epsilon:  0.9436225637280606\n",
      "episode completed, board position is\n",
      "58 \t: Episode || Reward:  -410 \t|| Average Reward:  -405.1186440677966 \t epsilon:  0.9426789411643326\n",
      "episode completed, board position is\n",
      "59 \t: Episode || Reward:  -348 \t|| Average Reward:  -404.1666666666667 \t epsilon:  0.9417362622231683\n",
      "episode completed, board position is\n",
      "60 \t: Episode || Reward:  -404 \t|| Average Reward:  -404.1639344262295 \t epsilon:  0.9407945259609451\n",
      "episode completed, board position is\n",
      "61 \t: Episode || Reward:  -400 \t|| Average Reward:  -404.0967741935484 \t epsilon:  0.9398537314349842\n",
      "episode completed, board position is\n",
      "62 \t: Episode || Reward:  -406 \t|| Average Reward:  -404.12698412698415 \t epsilon:  0.9389138777035492\n",
      "episode completed, board position is\n",
      "63 \t: Episode || Reward:  -412 \t|| Average Reward:  -404.25 \t epsilon:  0.9379749638258457\n",
      "episode completed, board position is\n",
      "64 \t: Episode || Reward:  -410 \t|| Average Reward:  -404.33846153846156 \t epsilon:  0.9370369888620198\n",
      "episode completed, board position is\n",
      "65 \t: Episode || Reward:  -414 \t|| Average Reward:  -404.4848484848485 \t epsilon:  0.9360999518731578\n",
      "episode completed, board position is\n",
      "66 \t: Episode || Reward:  -400 \t|| Average Reward:  -404.4179104477612 \t epsilon:  0.9351638519212846\n",
      "episode completed, board position is\n",
      "67 \t: Episode || Reward:  -446 \t|| Average Reward:  -405.02941176470586 \t epsilon:  0.9342286880693633\n",
      "episode completed, board position is\n",
      "68 \t: Episode || Reward:  -436 \t|| Average Reward:  -405.4782608695652 \t epsilon:  0.933294459381294\n",
      "episode completed, board position is\n",
      "69 \t: Episode || Reward:  -392 \t|| Average Reward:  -405.2857142857143 \t epsilon:  0.9323611649219127\n",
      "episode completed, board position is\n",
      "70 \t: Episode || Reward:  -402 \t|| Average Reward:  -405.23943661971833 \t epsilon:  0.9314288037569908\n",
      "episode completed, board position is\n",
      "71 \t: Episode || Reward:  -404 \t|| Average Reward:  -405.22222222222223 \t epsilon:  0.9304973749532338\n",
      "episode completed, board position is\n",
      "72 \t: Episode || Reward:  -414 \t|| Average Reward:  -405.3424657534247 \t epsilon:  0.9295668775782806\n",
      "episode completed, board position is\n",
      "73 \t: Episode || Reward:  -394 \t|| Average Reward:  -405.18918918918916 \t epsilon:  0.9286373107007023\n",
      "episode completed, board position is\n",
      "74 \t: Episode || Reward:  -426 \t|| Average Reward:  -405.46666666666664 \t epsilon:  0.9277086733900016\n",
      "episode completed, board position is\n",
      "75 \t: Episode || Reward:  -428 \t|| Average Reward:  -405.7631578947368 \t epsilon:  0.9267809647166116\n",
      "episode completed, board position is\n",
      "76 \t: Episode || Reward:  -416 \t|| Average Reward:  -405.8961038961039 \t epsilon:  0.925854183751895\n",
      "episode completed, board position is\n",
      "77 \t: Episode || Reward:  -422 \t|| Average Reward:  -406.1025641025641 \t epsilon:  0.9249283295681431\n",
      "episode completed, board position is\n",
      "78 \t: Episode || Reward:  -408 \t|| Average Reward:  -406.126582278481 \t epsilon:  0.9240034012385749\n",
      "episode completed, board position is\n",
      "79 \t: Episode || Reward:  -410 \t|| Average Reward:  -406.175 \t epsilon:  0.9230793978373364\n",
      "episode completed, board position is\n",
      "80 \t: Episode || Reward:  -416 \t|| Average Reward:  -406.2962962962963 \t epsilon:  0.9221563184394991\n",
      "episode completed, board position is\n",
      "81 \t: Episode || Reward:  -414 \t|| Average Reward:  -406.390243902439 \t epsilon:  0.9212341621210596\n",
      "episode completed, board position is\n",
      "82 \t: Episode || Reward:  -420 \t|| Average Reward:  -406.5542168674699 \t epsilon:  0.9203129279589385\n",
      "episode completed, board position is\n",
      "83 \t: Episode || Reward:  -398 \t|| Average Reward:  -406.45238095238096 \t epsilon:  0.9193926150309796\n",
      "episode completed, board position is\n",
      "84 \t: Episode || Reward:  -402 \t|| Average Reward:  -406.4 \t epsilon:  0.9184732224159486\n",
      "episode completed, board position is\n",
      "85 \t: Episode || Reward:  -418 \t|| Average Reward:  -406.5348837209302 \t epsilon:  0.9175547491935327\n",
      "episode completed, board position is\n",
      "86 \t: Episode || Reward:  -428 \t|| Average Reward:  -406.7816091954023 \t epsilon:  0.9166371944443392\n",
      "episode completed, board position is\n",
      "87 \t: Episode || Reward:  -438 \t|| Average Reward:  -407.1363636363636 \t epsilon:  0.9157205572498949\n",
      "episode completed, board position is\n",
      "88 \t: Episode || Reward:  -416 \t|| Average Reward:  -407.23595505617976 \t epsilon:  0.914804836692645\n",
      "episode completed, board position is\n",
      "89 \t: Episode || Reward:  -406 \t|| Average Reward:  -407.22222222222223 \t epsilon:  0.9138900318559524\n",
      "episode completed, board position is\n",
      "90 \t: Episode || Reward:  -438 \t|| Average Reward:  -407.56043956043953 \t epsilon:  0.9129761418240965\n",
      "episode completed, board position is\n",
      "91 \t: Episode || Reward:  -432 \t|| Average Reward:  -407.82608695652175 \t epsilon:  0.9120631656822724\n",
      "episode completed, board position is\n",
      "92 \t: Episode || Reward:  -406 \t|| Average Reward:  -407.80645161290323 \t epsilon:  0.9111511025165902\n",
      "episode completed, board position is\n",
      "93 \t: Episode || Reward:  -414 \t|| Average Reward:  -407.8723404255319 \t epsilon:  0.9102399514140735\n",
      "episode completed, board position is\n",
      "94 \t: Episode || Reward:  -420 \t|| Average Reward:  -408.0 \t epsilon:  0.9093297114626595\n",
      "episode completed, board position is\n",
      "95 \t: Episode || Reward:  -418 \t|| Average Reward:  -408.1041666666667 \t epsilon:  0.9084203817511969\n",
      "episode completed, board position is\n",
      "96 \t: Episode || Reward:  -404 \t|| Average Reward:  -408.0618556701031 \t epsilon:  0.9075119613694457\n",
      "episode completed, board position is\n",
      "97 \t: Episode || Reward:  -434 \t|| Average Reward:  -408.3265306122449 \t epsilon:  0.9066044494080763\n",
      "episode completed, board position is\n",
      "98 \t: Episode || Reward:  -404 \t|| Average Reward:  -408.2828282828283 \t epsilon:  0.9056978449586682\n",
      "episode completed, board position is\n",
      "99 \t: Episode || Reward:  -428 \t|| Average Reward:  -408.48 \t epsilon:  0.9047921471137096\n",
      "rewards earned so far is:  0\n",
      "\n",
      "\n",
      "\n",
      "BL - BL - BL - BL - BL - BL - BL - BL -  \n",
      "\n",
      "BL - bp2 - bp3 - bp4 - bp5 - bp6 - bp7 - bp8 -  \n",
      "\n",
      "BL - BL - BL - BL - BL - BL - BL - BL -  \n",
      "\n",
      "bp1 - BL - BL - BL - BL - BL - BL - BL -  \n",
      "\n",
      "BL - BL - BL - BL - BL - BL - BL - BL -  \n",
      "\n",
      "wp1 - wp2 - BL - BL - wp5 - BL - BL - BL -  \n",
      "\n",
      "BL - BL - wp3 - wp4 - BL - wp6 - wp7 - wp8 -  \n",
      "\n",
      "BL - BL - BL - BL - BL - BL - BL - BL -  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rewards earned so far is:  -168\n",
      "rewards earned so far is:  -320\n",
      "episode completed, board position is\n",
      "100 \t: Episode || Reward:  -390 \t|| Average Reward:  -408.42 \t epsilon:  0.9038873549665959\n",
      "episode completed, board position is\n",
      "101 \t: Episode || Reward:  -396 \t|| Average Reward:  -408.42 \t epsilon:  0.9029834676116293\n",
      "episode completed, board position is\n",
      "102 \t: Episode || Reward:  -422 \t|| Average Reward:  -408.5 \t epsilon:  0.9020804841440176\n",
      "episode completed, board position is\n",
      "103 \t: Episode || Reward:  -430 \t|| Average Reward:  -408.64 \t epsilon:  0.9011784036598737\n",
      "episode completed, board position is\n",
      "104 \t: Episode || Reward:  -394 \t|| Average Reward:  -408.52 \t epsilon:  0.9002772252562138\n",
      "episode completed, board position is\n",
      "105 \t: Episode || Reward:  -418 \t|| Average Reward:  -408.4 \t epsilon:  0.8993769480309576\n",
      "episode completed, board position is\n",
      "106 \t: Episode || Reward:  -426 \t|| Average Reward:  -408.38 \t epsilon:  0.8984775710829266\n",
      "episode completed, board position is\n",
      "107 \t: Episode || Reward:  -426 \t|| Average Reward:  -408.6 \t epsilon:  0.8975790935118436\n",
      "episode completed, board position is\n",
      "108 \t: Episode || Reward:  -402 \t|| Average Reward:  -408.66 \t epsilon:  0.8966815144183318\n",
      "episode completed, board position is\n",
      "109 \t: Episode || Reward:  -398 \t|| Average Reward:  -408.94 \t epsilon:  0.8957848329039134\n",
      "episode completed, board position is\n",
      "110 \t: Episode || Reward:  -430 \t|| Average Reward:  -409.34 \t epsilon:  0.8948890480710096\n",
      "episode completed, board position is\n",
      "111 \t: Episode || Reward:  -420 \t|| Average Reward:  -409.58 \t epsilon:  0.8939941590229386\n",
      "episode completed, board position is\n",
      "112 \t: Episode || Reward:  -442 \t|| Average Reward:  -409.88 \t epsilon:  0.8931001648639156\n",
      "episode completed, board position is\n",
      "113 \t: Episode || Reward:  -428 \t|| Average Reward:  -410.36 \t epsilon:  0.8922070646990518\n",
      "episode completed, board position is\n",
      "114 \t: Episode || Reward:  -418 \t|| Average Reward:  -410.62 \t epsilon:  0.8913148576343527\n",
      "episode completed, board position is\n",
      "115 \t: Episode || Reward:  -440 \t|| Average Reward:  -410.9 \t epsilon:  0.8904235427767183\n",
      "episode completed, board position is\n",
      "116 \t: Episode || Reward:  -408 \t|| Average Reward:  -411.0 \t epsilon:  0.8895331192339416\n",
      "episode completed, board position is\n",
      "117 \t: Episode || Reward:  -428 \t|| Average Reward:  -411.08 \t epsilon:  0.8886435861147077\n",
      "episode completed, board position is\n",
      "118 \t: Episode || Reward:  -416 \t|| Average Reward:  -411.42 \t epsilon:  0.887754942528593\n",
      "episode completed, board position is\n",
      "119 \t: Episode || Reward:  -374 \t|| Average Reward:  -410.98 \t epsilon:  0.8868671875860644\n",
      "episode completed, board position is\n",
      "120 \t: Episode || Reward:  -420 \t|| Average Reward:  -411.1 \t epsilon:  0.8859803203984784\n",
      "episode completed, board position is\n",
      "121 \t: Episode || Reward:  -406 \t|| Average Reward:  -410.78 \t epsilon:  0.88509434007808\n",
      "episode completed, board position is\n",
      "122 \t: Episode || Reward:  -446 \t|| Average Reward:  -411.26 \t epsilon:  0.8842092457380019\n",
      "episode completed, board position is\n",
      "123 \t: Episode || Reward:  -428 \t|| Average Reward:  -411.54 \t epsilon:  0.8833250364922639\n",
      "episode completed, board position is\n",
      "124 \t: Episode || Reward:  -412 \t|| Average Reward:  -411.84 \t epsilon:  0.8824417114557717\n",
      "episode completed, board position is\n",
      "125 \t: Episode || Reward:  -406 \t|| Average Reward:  -412.28 \t epsilon:  0.8815592697443159\n",
      "episode completed, board position is\n",
      "126 \t: Episode || Reward:  -422 \t|| Average Reward:  -412.58 \t epsilon:  0.8806777104745716\n",
      "episode completed, board position is\n",
      "127 \t: Episode || Reward:  -406 \t|| Average Reward:  -412.3 \t epsilon:  0.8797970327640969\n",
      "episode completed, board position is\n",
      "128 \t: Episode || Reward:  -406 \t|| Average Reward:  -412.02 \t epsilon:  0.8789172357313328\n",
      "episode completed, board position is\n",
      "129 \t: Episode || Reward:  -418 \t|| Average Reward:  -412.4 \t epsilon:  0.8780383184956015\n",
      "episode completed, board position is\n",
      "130 \t: Episode || Reward:  -428 \t|| Average Reward:  -412.6 \t epsilon:  0.8771602801771059\n",
      "episode completed, board position is\n",
      "131 \t: Episode || Reward:  -422 \t|| Average Reward:  -413.12 \t epsilon:  0.8762831198969288\n",
      "episode completed, board position is\n",
      "132 \t: Episode || Reward:  -418 \t|| Average Reward:  -413.28 \t epsilon:  0.8754068367770318\n",
      "episode completed, board position is\n",
      "133 \t: Episode || Reward:  -432 \t|| Average Reward:  -413.66 \t epsilon:  0.8745314299402548\n",
      "episode completed, board position is\n",
      "134 \t: Episode || Reward:  -412 \t|| Average Reward:  -413.62 \t epsilon:  0.8736568985103146\n",
      "episode completed, board position is\n",
      "135 \t: Episode || Reward:  -442 \t|| Average Reward:  -413.9 \t epsilon:  0.8727832416118043\n",
      "episode completed, board position is\n",
      "136 \t: Episode || Reward:  -432 \t|| Average Reward:  -414.38 \t epsilon:  0.8719104583701925\n",
      "episode completed, board position is\n",
      "137 \t: Episode || Reward:  -408 \t|| Average Reward:  -414.48 \t epsilon:  0.8710385479118223\n",
      "episode completed, board position is\n",
      "138 \t: Episode || Reward:  -414 \t|| Average Reward:  -414.48 \t epsilon:  0.8701675093639105\n",
      "episode completed, board position is\n",
      "139 \t: Episode || Reward:  -428 \t|| Average Reward:  -414.8 \t epsilon:  0.8692973418545467\n",
      "episode completed, board position is\n",
      "140 \t: Episode || Reward:  -436 \t|| Average Reward:  -415.32 \t epsilon:  0.8684280445126921\n",
      "episode completed, board position is\n",
      "141 \t: Episode || Reward:  -424 \t|| Average Reward:  -415.24 \t epsilon:  0.8675596164681794\n",
      "episode completed, board position is\n",
      "142 \t: Episode || Reward:  -388 \t|| Average Reward:  -415.0 \t epsilon:  0.8666920568517111\n",
      "episode completed, board position is\n",
      "143 \t: Episode || Reward:  -434 \t|| Average Reward:  -415.56 \t epsilon:  0.8658253647948594\n",
      "episode completed, board position is\n",
      "144 \t: Episode || Reward:  -426 \t|| Average Reward:  -415.72 \t epsilon:  0.8649595394300645\n",
      "episode completed, board position is\n",
      "145 \t: Episode || Reward:  -414 \t|| Average Reward:  -415.5 \t epsilon:  0.8640945798906344\n",
      "episode completed, board position is\n",
      "146 \t: Episode || Reward:  -416 \t|| Average Reward:  -415.52 \t epsilon:  0.8632304853107438\n",
      "episode completed, board position is\n",
      "147 \t: Episode || Reward:  -436 \t|| Average Reward:  -415.62 \t epsilon:  0.862367254825433\n",
      "episode completed, board position is\n",
      "148 \t: Episode || Reward:  -432 \t|| Average Reward:  -415.76 \t epsilon:  0.8615048875706075\n",
      "episode completed, board position is\n",
      "149 \t: Episode || Reward:  -436 \t|| Average Reward:  -416.06 \t epsilon:  0.8606433826830369\n",
      "rewards earned so far is:  0\n",
      "\n",
      "\n",
      "\n",
      "BL - BL - BL - BL - BL - BL - BL - BL -  \n",
      "\n",
      "bp1 - bp2 - BL - bp4 - bp5 - BL - bp7 - bp8 -  \n",
      "\n",
      "BL - BL - BL - BL - BL - bp6 - BL - BL -  \n",
      "\n",
      "BL - BL - bp3 - BL - BL - BL - BL - BL -  \n",
      "\n",
      "BL - BL - BL - BL - BL - BL - BL - BL -  \n",
      "\n",
      "BL - wp2 - BL - wp4 - BL - BL - BL - BL -  \n",
      "\n",
      "wp1 - BL - wp3 - BL - wp5 - wp6 - wp7 - wp8 -  \n",
      "\n",
      "BL - BL - BL - BL - BL - BL - BL - BL -  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rewards earned so far is:  -178\n",
      "rewards earned so far is:  -360\n",
      "episode completed, board position is\n",
      "150 \t: Episode || Reward:  -432 \t|| Average Reward:  -416.04 \t epsilon:  0.8597827393003539\n",
      "episode completed, board position is\n",
      "151 \t: Episode || Reward:  -396 \t|| Average Reward:  -415.66 \t epsilon:  0.8589229565610536\n",
      "episode completed, board position is\n",
      "152 \t: Episode || Reward:  -398 \t|| Average Reward:  -415.26 \t epsilon:  0.8580640336044925\n",
      "episode completed, board position is\n",
      "153 \t: Episode || Reward:  -446 \t|| Average Reward:  -415.72 \t epsilon:  0.857205969570888\n",
      "episode completed, board position is\n",
      "154 \t: Episode || Reward:  -426 \t|| Average Reward:  -415.94 \t epsilon:  0.8563487636013172\n",
      "episode completed, board position is\n",
      "155 \t: Episode || Reward:  -416 \t|| Average Reward:  -416.56 \t epsilon:  0.8554924148377159\n",
      "episode completed, board position is\n",
      "156 \t: Episode || Reward:  -416 \t|| Average Reward:  -416.52 \t epsilon:  0.8546369224228781\n",
      "episode completed, board position is\n",
      "157 \t: Episode || Reward:  -444 \t|| Average Reward:  -416.84 \t epsilon:  0.8537822855004553\n",
      "episode completed, board position is\n",
      "158 \t: Episode || Reward:  -404 \t|| Average Reward:  -416.78 \t epsilon:  0.8529285032149548\n",
      "episode completed, board position is\n",
      "159 \t: Episode || Reward:  -404 \t|| Average Reward:  -417.34 \t epsilon:  0.8520755747117399\n",
      "episode completed, board position is\n",
      "160 \t: Episode || Reward:  -408 \t|| Average Reward:  -417.38 \t epsilon:  0.8512234991370281\n",
      "episode completed, board position is\n",
      "161 \t: Episode || Reward:  -422 \t|| Average Reward:  -417.6 \t epsilon:  0.8503722756378911\n",
      "episode completed, board position is\n",
      "162 \t: Episode || Reward:  -430 \t|| Average Reward:  -417.84 \t epsilon:  0.8495219033622532\n",
      "episode completed, board position is\n",
      "163 \t: Episode || Reward:  -374 \t|| Average Reward:  -417.46 \t epsilon:  0.8486723814588909\n",
      "episode completed, board position is\n",
      "164 \t: Episode || Reward:  -414 \t|| Average Reward:  -417.5 \t epsilon:  0.847823709077432\n",
      "episode completed, board position is\n",
      "165 \t: Episode || Reward:  -428 \t|| Average Reward:  -417.64 \t epsilon:  0.8469758853683546\n",
      "episode completed, board position is\n",
      "166 \t: Episode || Reward:  -420 \t|| Average Reward:  -417.84 \t epsilon:  0.8461289094829862\n",
      "episode completed, board position is\n",
      "167 \t: Episode || Reward:  -432 \t|| Average Reward:  -417.7 \t epsilon:  0.8452827805735033\n",
      "episode completed, board position is\n",
      "168 \t: Episode || Reward:  -414 \t|| Average Reward:  -417.48 \t epsilon:  0.8444374977929298\n",
      "episode completed, board position is\n",
      "169 \t: Episode || Reward:  -432 \t|| Average Reward:  -417.88 \t epsilon:  0.8435930602951368\n",
      "episode completed, board position is\n",
      "170 \t: Episode || Reward:  -418 \t|| Average Reward:  -418.04 \t epsilon:  0.8427494672348417\n",
      "episode completed, board position is\n",
      "171 \t: Episode || Reward:  -416 \t|| Average Reward:  -418.16 \t epsilon:  0.8419067177676068\n",
      "episode completed, board position is\n",
      "172 \t: Episode || Reward:  -438 \t|| Average Reward:  -418.4 \t epsilon:  0.8410648110498392\n",
      "episode completed, board position is\n",
      "173 \t: Episode || Reward:  -436 \t|| Average Reward:  -418.82 \t epsilon:  0.8402237462387894\n",
      "episode completed, board position is\n",
      "174 \t: Episode || Reward:  -412 \t|| Average Reward:  -418.68 \t epsilon:  0.8393835224925505\n",
      "episode completed, board position is\n",
      "175 \t: Episode || Reward:  -422 \t|| Average Reward:  -418.62 \t epsilon:  0.838544138970058\n",
      "episode completed, board position is\n",
      "176 \t: Episode || Reward:  -404 \t|| Average Reward:  -418.5 \t epsilon:  0.8377055948310879\n",
      "episode completed, board position is\n",
      "177 \t: Episode || Reward:  -404 \t|| Average Reward:  -418.32 \t epsilon:  0.8368678892362568\n",
      "episode completed, board position is\n",
      "178 \t: Episode || Reward:  -420 \t|| Average Reward:  -418.44 \t epsilon:  0.8360310213470206\n",
      "episode completed, board position is\n",
      "179 \t: Episode || Reward:  -410 \t|| Average Reward:  -418.44 \t epsilon:  0.8351949903256736\n",
      "episode completed, board position is\n",
      "180 \t: Episode || Reward:  -422 \t|| Average Reward:  -418.5 \t epsilon:  0.8343597953353479\n",
      "episode completed, board position is\n",
      "181 \t: Episode || Reward:  -438 \t|| Average Reward:  -418.74 \t epsilon:  0.8335254355400126\n",
      "episode completed, board position is\n",
      "182 \t: Episode || Reward:  -402 \t|| Average Reward:  -418.56 \t epsilon:  0.8326919101044725\n",
      "episode completed, board position is\n",
      "183 \t: Episode || Reward:  -410 \t|| Average Reward:  -418.68 \t epsilon:  0.831859218194368\n",
      "episode completed, board position is\n",
      "184 \t: Episode || Reward:  -434 \t|| Average Reward:  -419.0 \t epsilon:  0.8310273589761736\n",
      "episode completed, board position is\n",
      "185 \t: Episode || Reward:  -426 \t|| Average Reward:  -419.08 \t epsilon:  0.8301963316171974\n",
      "episode completed, board position is\n",
      "186 \t: Episode || Reward:  -406 \t|| Average Reward:  -418.86 \t epsilon:  0.8293661352855802\n",
      "episode completed, board position is\n",
      "187 \t: Episode || Reward:  -416 \t|| Average Reward:  -418.64 \t epsilon:  0.8285367691502946\n",
      "episode completed, board position is\n",
      "188 \t: Episode || Reward:  -400 \t|| Average Reward:  -418.48 \t epsilon:  0.8277082323811443\n",
      "episode completed, board position is\n",
      "189 \t: Episode || Reward:  -426 \t|| Average Reward:  -418.68 \t epsilon:  0.8268805241487632\n",
      "episode completed, board position is\n",
      "190 \t: Episode || Reward:  -412 \t|| Average Reward:  -418.42 \t epsilon:  0.8260536436246144\n",
      "episode completed, board position is\n",
      "191 \t: Episode || Reward:  -406 \t|| Average Reward:  -418.16 \t epsilon:  0.8252275899809898\n",
      "episode completed, board position is\n",
      "192 \t: Episode || Reward:  -422 \t|| Average Reward:  -418.32 \t epsilon:  0.8244023623910088\n",
      "episode completed, board position is\n",
      "193 \t: Episode || Reward:  -396 \t|| Average Reward:  -418.14 \t epsilon:  0.8235779600286178\n",
      "episode completed, board position is\n",
      "194 \t: Episode || Reward:  -450 \t|| Average Reward:  -418.44 \t epsilon:  0.8227543820685892\n",
      "episode completed, board position is\n",
      "195 \t: Episode || Reward:  -418 \t|| Average Reward:  -418.44 \t epsilon:  0.8219316276865206\n",
      "episode completed, board position is\n",
      "196 \t: Episode || Reward:  -408 \t|| Average Reward:  -418.48 \t epsilon:  0.8211096960588341\n",
      "episode completed, board position is\n",
      "197 \t: Episode || Reward:  -410 \t|| Average Reward:  -418.24 \t epsilon:  0.8202885863627752\n",
      "episode completed, board position is\n",
      "198 \t: Episode || Reward:  -422 \t|| Average Reward:  -418.42 \t epsilon:  0.8194682977764125\n",
      "episode completed, board position is\n",
      "199 \t: Episode || Reward:  -420 \t|| Average Reward:  -418.34 \t epsilon:  0.818648829478636\n"
     ]
    }
   ],
   "source": [
    "    rewards_list = []\n",
    "\n",
    "    # Run 100 episodes to generate the initial training data\n",
    "    #num_test_episode = 100\n",
    "\n",
    "    # Create the OpenAI Gym Enironment with LunarLander-v2\n",
    "    env = ChessBoard_gym()\n",
    "\n",
    "    # set the numpy random number generatorseeds\n",
    "    env.seed(21)\n",
    "    np.random.seed(21)\n",
    "\n",
    "    # max number of training episodes\n",
    "    training_episodes = 200\n",
    "\n",
    "    # number of test runs with a satisfactory number of good landings\n",
    "    #high_score = 0\n",
    " \n",
    "    # initialize the Deep-Q Network model\n",
    "    model = DQN(env)\n",
    "\n",
    "    # Train the model\n",
    "    model.train(training_episodes, True)\n",
    "\n",
    "    now = datetime.now()\n",
    "    date_time = now.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    model.save('model/chess-reinf_model' + date_time + '.h5')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Testing of the trained model...\n",
      "episode completed, board position is\n",
      "0 \t: Episode || Reward:  -500\n",
      "\n",
      "\n",
      "\n",
      "BL - BL - BL - BL - BL - BL - BL - BL -  \n",
      "\n",
      "bp1 - bp2 - bp3 - bp4 - bp5 - bp6 - bp7 - bp8 -  \n",
      "\n",
      "BL - BL - BL - BL - BL - BL - BL - BL -  \n",
      "\n",
      "BL - BL - BL - BL - BL - BL - BL - BL -  \n",
      "\n",
      "BL - BL - BL - BL - BL - BL - BL - BL -  \n",
      "\n",
      "BL - BL - BL - BL - BL - BL - BL - BL -  \n",
      "\n",
      "wp1 - wp2 - wp3 - wp4 - wp5 - wp6 - wp7 - wp8 -  \n",
      "\n",
      "BL - BL - BL - BL - BL - BL - BL - BL -  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode completed, board position is\n",
      "1 \t: Episode || Reward:  -500\n",
      "episode completed, board position is\n",
      "2 \t: Episode || Reward:  -500\n",
      "episode completed, board position is\n",
      "3 \t: Episode || Reward:  -500\n",
      "episode completed, board position is\n",
      "4 \t: Episode || Reward:  -500\n",
      "Average Reward:  -500.0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'high_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-36f880dc3688>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0mrewards_mean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Average Reward: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards_mean\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Total tests above 200: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhigh_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'high_score' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Starting Testing of the trained model...\")\n",
    "\n",
    "done = False\n",
    "frames = []\n",
    "num_test_episode = 5\n",
    "num_observation_space = 64\n",
    "high_score = 0\n",
    "\n",
    "# Run some test episodes to see how well our model performs\n",
    "for test_episode in range(num_test_episode):\n",
    "    current_state = env.reset()\n",
    "#     num_observation_space = env.observation_space.shape[0]\n",
    "    current_state = np.reshape(current_state, [1, num_observation_space])\n",
    "    reward_for_episode = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "\n",
    "        frame = env.get_FEN()\n",
    "        frames.append(frame)\n",
    "\n",
    "        selected_action = np.argmax(model.predict(current_state)[0])\n",
    "        new_state, reward, done, info = env.step(selected_action)\n",
    "        new_state = np.reshape(new_state, [1, num_observation_space])\n",
    "        current_state = new_state\n",
    "        reward_for_episode += reward\n",
    "    rewards_list.append(reward_for_episode)\n",
    "    print(test_episode, \"\\t: Episode || Reward: \", reward_for_episode)\n",
    "    if reward_for_episode >= 200:\n",
    "        high_score += 1\n",
    "    if test_episode % 100 == 0:\n",
    "        env.print_board()\n",
    "\n",
    "\n",
    "now = datetime.now() # current date and time\n",
    "rewards_mean = np.mean(rewards_list[-100:])\n",
    "print(\"Average Reward: \", rewards_mean )\n",
    "print(\"Total tests above 200: \", high_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_observation_space, current_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
